#include "../ShaderLibrary/Common.hlsl"
#pragma kernel PureDepthSSAO
#pragma kernel SSAO
#pragma kernel HBAO
#pragma kernel GTAO
#pragma kernel NormalBilateralFilter0
#pragma kernel NormalBilateralFilter1
#pragma kernel AdaptionBilateralUpSample
#pragma kernel AdaptionBilateralFilter0
#pragma kernel AdaptionBilateralFilter1
#pragma kernel DebugAO
#pragma kernel DebugRO

RWTexture2D<float3> AmbientOcclusionRT;
RWTexture2D<float3> UpSampleRT;
RWTexture2D<float3> Filtered0;
RWTexture2D<float3> Filtered1;
Texture2D<float3> RandomTexture;
Texture2D<float3> aoResult;
RWTexture2D<float3> debugResult;
float4x4 _CameraProjection_SSAO;
float4x4 _CameraInverseProjection_SSAO;
//shared properties
int sampleCount;
int downSample;
float aoRadius;
float2 filterRadius;
float filterFactor;
int kernelSize;
//pure depth method properties
float threshold;
float area;
float strength;
float correction;
//ssao method properties
float bias;
float contrast;
float magnitude;
float power;
//hbao method properties
float _CameraNearPlane;
int numDirection;
int maxRadiusPixel;
float tanBias;
float hbaoStrength;
//gtao method properties
float _HalfProjScale;
float4 fadeParams;
int numSlice;
float thickness;
float gtaoStrength;
int multipleBounce;

//refer to : https://theorangeduck.com/page/pure-depth-ssao
float3 normalFromDepth(float depth, float2 texCoords) {
    const float2 offset1 = float2(0.0, 0.001);
    const float2 offset2 = float2(0.001, 0.0);
    float depth1 = LinearEyeDepth(SAMPLE_TEXTURE2D_LOD(_CameraDepthTexture, sampler_point_clamp, texCoords + offset1, 0), _ZBufferParams);
    float depth2 = LinearEyeDepth(SAMPLE_TEXTURE2D_LOD(_CameraDepthTexture, sampler_point_clamp, texCoords + offset2, 0), _ZBufferParams);
    float3 p1 = float3(offset1, depth1 - depth);
    float3 p2 = float3(offset2, depth2 - depth);
    float3 normal = cross(p1, p2);
    normal.z = -normal.z;
    return normalize(normal);
}

float3 UvToViewSpace(float2 uv, float depth) {
    //calculate camera to far plane ray in the screen
    float4 cameraRay = float4(uv * 2.0 - 1.0, 1.0, 1.0);
    cameraRay = mul(_CameraInverseProjection_SSAO, cameraRay);
    cameraRay.xyz = cameraRay.xyz / cameraRay.w;
    //linear depth 0-1
    return cameraRay.xyz * depth;
}

[numthreads(8,8,1)]
void PureDepthSSAO(uint3 id : SV_DispatchThreadID) {
    float3 sample_sphere[16] = {
        float3(0.5381, 0.1856,-0.4319), float3(0.1379, 0.2486, 0.4430),
        float3(0.3371, 0.5679,-0.0057), float3(-0.6999,-0.0451,-0.0019),
        float3(0.0689,-0.1598,-0.8547), float3(0.0560, 0.0069,-0.1843),
        float3(-0.0146, 0.1402, 0.0762), float3(0.0100,-0.1924,-0.0344),
        float3(-0.3577,-0.5301,-0.4358), float3(-0.3169, 0.1063, 0.0158),
        float3(0.0103,-0.5869, 0.0046), float3(-0.0897,-0.4940, 0.3287),
        float3(0.7119,-0.0154,-0.0918), float3(-0.0533, 0.0596,-0.5411),
        float3(0.0352,-0.0631, 0.5460), float3(-0.4776, 0.2847,-0.0271)
    };
    //[0,_CameraBufferSize-1] -> screen [0,1] uv
    float2 screenUV = id.xy / (_CameraBufferSize.zw / downSample);
    float rawDepth = SAMPLE_TEXTURE2D_LOD(_CameraDepthTexture, sampler_point_clamp, screenUV, 0);
    float linearDepth = LinearEyeDepth(rawDepth, _ZBufferParams);
    float3 random = normalize(SAMPLE_TEXTURE2D_LOD(RandomTexture, sampler_linear_clamp, screenUV, 0).rgb);
    random = normalize(SAMPLE_TEXTURE2D_LOD(RandomTexture, sampler_linear_repeat, screenUV + random, 0).rgb);
    float3 position = float3(screenUV, linearDepth);
    float3 normal = normalFromDepth(linearDepth, screenUV);
    //near big far small
    float sampleRadius = aoRadius / linearDepth;
    float occlusion = 0.0;
    [loop]
    for (int i = 0; i < 16; i++) {
        float3 ray = sampleRadius * reflect(sample_sphere[i], random);
        float3 hemiRay = position + sign(dot(ray, normal)) * ray;
        float occlusionDepth = LinearEyeDepth(SAMPLE_TEXTURE2D_LOD(_CameraDepthTexture, sampler_point_clamp, saturate(hemiRay.xy), 0), _ZBufferParams);
        float difference = linearDepth - occlusionDepth;
        occlusion += step(threshold, difference) * (1.0 - smoothstep(threshold, area, length(ray)));
    }
    float ao = 1 - strength * occlusion / 16;
    ao = saturate(ao + correction);
    AmbientOcclusionRT[id.xy] = float3(ao, 0, 0);
}

//there are many methods about SSAO,such as sphere-sample based、hemisphere-sample based(cos angle weight) and linear-sample based(distance weight), this is the simple sphere-sample based method
[numthreads(8, 8, 1)]
void SSAO(uint3 id : SV_DispatchThreadID) {
    float3 sample_sphere[16] = {
        float3(0.5381, 0.1856,-0.4319), float3(0.1379, 0.2486, 0.4430),
        float3(0.3371, 0.5679,-0.0057), float3(-0.6999,-0.0451,-0.0019),
        float3(0.0689,-0.1598,-0.8547), float3(0.0560, 0.0069,-0.1843),
        float3(-0.0146, 0.1402, 0.0762), float3(0.0100,-0.1924,-0.0344),
        float3(-0.3577,-0.5301,-0.4358), float3(-0.3169, 0.1063, 0.0158),
        float3(0.0103,-0.5869, 0.0046), float3(-0.0897,-0.4940, 0.3287),
        float3(0.7119,-0.0154,-0.0918), float3(-0.0533, 0.0596,-0.5411),
        float3(0.0352,-0.0631, 0.5460), float3(-0.4776, 0.2847,-0.0271)
    };
    float2 screenUV = id.xy / (_CameraBufferSize.zw / downSample);
    float4 depthNormalTexture = SAMPLE_TEXTURE2D_LOD(_CameraDepthNormalTexture, sampler_point_clamp, screenUV, 0);
    //depth of the DepthNormal is not realiabe, maybe sample Depth Texture instead
    float4 depthTexture = SAMPLE_DEPTH_TEXTURE_LOD(_CameraDepthTexture, sampler_point_clamp, screenUV, 0);
    float3 bufferNormal = DecodeViewNormalStereo(depthNormalTexture);
    //float bufferDepth = DecodeFloatRG(depthNormalTexture.zw);
    float bufferDepth = depthTexture.r;
    //Linear01Depth get [0,1] depth, LinearEyeDepth get [near, far] depth
    float linearDepth = LinearEyeDepth(bufferDepth, _ZBufferParams);
    float linear01Depth = Linear01Depth(bufferDepth, _ZBufferParams);
    //reconstruct view space position
    float3 bufferPosition = UvToViewSpace(screenUV, linear01Depth);
    float3 random = normalize(SAMPLE_TEXTURE2D_LOD(RandomTexture, sampler_linear_clamp, screenUV, 0).rgb);
    random = normalize(SAMPLE_TEXTURE2D_LOD(RandomTexture, sampler_linear_repeat, screenUV + random, 0).rgb);
    //NOTE : need this matrix to transform the sample vectors from tangent space to view space, but I do not know how
    float3 normal = normalize(bufferNormal);
    float3 tangent = normalize(random - normal * dot(random, normal));
    float3 binormal = cross(bufferNormal, tangent);
    float3x3 tbn = float3x3(tangent, binormal, normal);
    float occlusion = 0.0;
    [loop]
    for (int i = 0; i < 16; i++) {
        float3 ray = aoRadius * normalize(reflect(sample_sphere[i], random));
        ray = mul(tbn, ray);
        //invert positive direction ray
        float3 samplePositionVS = bufferPosition + ray;
        //project sample point into clip space
        float4 samplePositionCS = mul(_CameraProjection_SSAO, float4(samplePositionVS, 1));
        samplePositionCS.xy /= samplePositionCS.w;
        samplePositionCS.xy *= 0.5;
        samplePositionCS.xy += 0.5;
        //get sample point scene depth
        float sampleDepth = SAMPLE_DEPTH_TEXTURE_LOD(_CameraDepthTexture, sampler_point_clamp, samplePositionCS.xy, 0);
        sampleDepth = LinearEyeDepth(sampleDepth, _ZBufferParams);
        float sampleDistance = -samplePositionVS.z;
        float difference = sampleDistance - sampleDepth;
        //weight by cos angle
        float weight = smoothstep(0.001, contrast, aoRadius / dot(ray, normal));
        occlusion +=  weight * step(0.001, difference);
    }
    float ao = 1 - magnitude * occlusion / 16;
    ao = saturate(ao + bias);
    ao = pow(ao, power);
    AmbientOcclusionRT[id.xy] = float3(ao, 0, 0);
}

float3 MinDiff(float3 p, float3 p1, float3 p2) {
    float3 v1 = p1 - p;
    float3 v2 = p - p2;
    return (length(v1) < length(v2)) ? v1 : v2;
}

float2 RotateDirections(float2 dir, float2 CosSin) {
    return float2(
        dir.x * CosSin.x - dir.y * CosSin.y, 
        dir.x * CosSin.y + dir.y * CosSin.x
        );
}

float2 SnapUVOffset(float2 uv, float2 bufferSize, float2 invBufferSize) {
    return floor(uv * bufferSize + 0.5) * invBufferSize;
}

float GetTangent(float3 v) {
    //tange(V) = Vz/Vxy
    return v.z * rsqrt(dot(v.xy, v.xy));
}

float TanToSin(float x) {
    return x * rsqrt(x * x + 1.0);
}

float FallOff(float distance) {
    return 1 - (distance * distance) / (aoRadius * aoRadius);
}

[numthreads(8, 8, 1)]
void HBAO(uint3 id : SV_DispatchThreadID) {
    float2 bufferSize = _CameraBufferSize.zw / downSample;
    float2 invBufferSize = _CameraBufferSize.xy * downSample;
    float2 screenUV = id.xy / bufferSize;
    float4 depthNormalTexture = SAMPLE_TEXTURE2D_LOD(_CameraDepthNormalTexture, sampler_point_clamp, screenUV, 0);
    //depth of the DepthNormal is not realiabe, maybe sample Depth Texture instead
    float4 depthTexture = SAMPLE_DEPTH_TEXTURE_LOD(_CameraDepthTexture, sampler_point_clamp, screenUV, 0);
    float3 bufferNormal = DecodeViewNormalStereo(depthNormalTexture);
    //float bufferDepth = DecodeFloatRG(depthNormalTexture.zw);
    float bufferDepth = depthTexture.r;
    //Linear01Depth get [0,1] depth, LinearEyeDepth get [near, far] depth
    float linearDepth = LinearEyeDepth(bufferDepth, _ZBufferParams);
    float linear01Depth = Linear01Depth(bufferDepth, _ZBufferParams);
    //point and surroundings
    float3 p, pr, pl, pt, pb;
    //scale this tiny tiny delta
    float2 delta = invBufferSize * 1.2;
    p = UvToViewSpace(screenUV, linear01Depth);
    linear01Depth = Linear01Depth(SAMPLE_DEPTH_TEXTURE_LOD(_CameraDepthTexture, sampler_point_clamp, screenUV + float2(delta.x, 0), 0) , _ZBufferParams);
    pr = UvToViewSpace(screenUV + float2(delta.x, 0), linear01Depth);
    linear01Depth = Linear01Depth(SAMPLE_DEPTH_TEXTURE_LOD(_CameraDepthTexture, sampler_point_clamp, screenUV + float2(-delta.x, 0), 0), _ZBufferParams);
    pl = UvToViewSpace(screenUV + float2(-delta.x, 0), linear01Depth);
    linear01Depth = Linear01Depth(SAMPLE_DEPTH_TEXTURE_LOD(_CameraDepthTexture, sampler_point_clamp, screenUV + float2(0, delta.y), 0), _ZBufferParams);
    pt = UvToViewSpace(screenUV + float2(0, delta.y), linear01Depth);
    linear01Depth = Linear01Depth(SAMPLE_DEPTH_TEXTURE_LOD(_CameraDepthTexture, sampler_point_clamp, screenUV + float2(0, -delta.y), 0), _ZBufferParams);
    pb = UvToViewSpace(screenUV + float2(0, -delta.y), linear01Depth);
    //get view space position delta of the point p in screen space
    float3 dpdu = MinDiff(p, pr, pl);
    float3 dpdv = MinDiff(p, pt, pb) * _CameraBufferSize.x * _CameraBufferSize.w;
    //random is used to offset sample direction
    float3 random = normalize(SAMPLE_TEXTURE2D_LOD(RandomTexture, sampler_linear_clamp, screenUV, 0).rgb);
    random = normalize(SAMPLE_TEXTURE2D_LOD(RandomTexture, sampler_linear_repeat, screenUV + random, 0).rgb);
    //calculate the projected size of the hemisphere
    float2 sampleRadiusUV = 0.5 * aoRadius * _CameraNearPlane / linearDepth;
    float radiusPixel = sampleRadiusUV.x * (_CameraBufferSize.zw / downSample);
    float ao = 1.0;
    //make sure the radius of the projected hemisphere is more than a pixel
    if (radiusPixel > 1.0) {
        //avoid oversampling if numSteps is greater than the kernel radius in pixels
        float numSteps = min(sampleCount, radiusPixel);
        //divide by Ns+1 so that the farthest samples are not fully attenuated(normally numSteps is equal to sampleCount)
        float stepSizePix = radiusPixel / (numSteps + 1);
        //clamp numSteps if it is greater than the max kernel footprint
        float maxNumSteps = maxRadiusPixel / stepSizePix;
        if (maxNumSteps < numSteps) {
            //use dithering to avoid AO discontinuities
            numSteps = floor(maxNumSteps + random.z);
            numSteps = max(numSteps, 1);
            stepSizePix = maxRadiusPixel / numSteps;
        }
        //step size in uv space
        float2 stepSize = stepSizePix * invBufferSize;
        //radius diff
        float alpha = 2.0 * PI / numDirection;
        //calculate the horizon occlusion of each direction
        [loop]
        for (int i = 0; i < numDirection; ++i) {
            float theta = alpha * i;
            //random offset the direction
            float2 dir = RotateDirections(float2(cos(theta), sin(theta)), random.xy);
            float2 deltaUV = dir * stepSize;
            //offset sample start point
            float2 uv = screenUV + SnapUVOffset(random.xy * deltaUV, bufferSize, invBufferSize);
            deltaUV = SnapUVOffset(deltaUV, bufferSize, invBufferSize);
            //calculate the tangent vector
            float3 T = deltaUV.x * dpdu + deltaUV.y * dpdv;
            //get the angle of tangent vector from the view space axis
            float tanT = GetTangent(T) + tanBias;
            float sinT = TanToSin(tanT);
            //get horizontal vector
            float3 H;
            float tanH;
            float dist;
            [loop]
            for (int j = 1; j <= sampleCount; ++j) {
                uv += deltaUV;
                float depth = SAMPLE_DEPTH_TEXTURE_LOD(_CameraDepthTexture, sampler_point_clamp, uv, 0);
                depth = Linear01Depth(depth, _ZBufferParams);
                H = UvToViewSpace(uv, depth);
                //horizontal ray from sample point
                tanH = GetTangent(H - p);
                dist = length(H - p);
                if (dist < aoRadius && tanH > tanT) {
                    float sinH = TanToSin(tanH);
                    ao += FallOff(dist) * (sinH - sinT);
                    tanT = tanH;
                    sinT = sinH;
                }
            }
        }
        ao = 1.0 - ao / numDirection * hbaoStrength;
    }

    AmbientOcclusionRT[id.xy] = float3(ao, 0, 0);
}

//refer to : Practical Realtime Strategies for Accurate Indirect Occlusion from siggraph 2016
float SpatialDirection(float2 position) {
    float noise = (1.0 / 16.0) * ((((int)(position.x + position.y) & 0x3) << 2) + ((int)position.x & 0x3));
    return noise;
}
//refer to : Practical Realtime Strategies for Accurate Indirect Occlusion from siggraph 2016
float SpatialOffset(float2 position) {
    float noise = (1.0 / 4.0) * ((int)(position.y - position.x) & 0x3);
    return noise;
}

float ComputeDistanceFade(float distance) {
    return saturate(max(0, distance - fadeParams.x) * fadeParams.y);
}

float IntegrateArc_UniformWeighting(float2 h) {
    float2 Arc = 1 - cos(h);
    return Arc.x + Arc.y;
}

float IntegrateArc_CosineWeighting(float2 h, float gamma) {
    //add cos(gamma) to make result 0 when gamma is 0
    float2 Arc = -cos(2 * h - gamma) + cos(gamma) + 2 * h * sin(gamma);
    return 0.25 * (Arc.x + Arc.y);
}

float ConeConeIntersection(float ArcLength0, float ArcLength1, float AngleBetweenCones) {
    float AngleDifference = abs(ArcLength0 - ArcLength1);
    float AngleBlendAlpha = saturate((AngleBetweenCones - AngleDifference) / (ArcLength0 + ArcLength1 - AngleDifference));
    return smoothstep(0, 1, 1 - AngleBlendAlpha);
}

float ReflectionOcclusion(float3 BentNormal, float3 ReflectionVector, float Roughness, float OcclusionStrength) {
    float BentNormalLength = length(BentNormal);
    float ReflectionConeAngle = max(Roughness, 0.04) * PI;
    float UnoccludedAngle = BentNormalLength * PI * OcclusionStrength;
    float AngleBetween = acos(dot(BentNormal, ReflectionVector) / max(BentNormalLength, 0.001));
    half ReflectionOcclusion = ConeConeIntersection(ReflectionConeAngle, UnoccludedAngle, AngleBetween);
    return lerp(0, ReflectionOcclusion, saturate((UnoccludedAngle - 0.1) / 0.2));
}

[numthreads(8, 8, 1)]
void GTAO(uint3 id : SV_DispatchThreadID) {
    float2 bufferSize = _CameraBufferSize.zw / downSample;
    float2 invBufferSize = _CameraBufferSize.xy * downSample;
    float2 screenUV = id.xy / bufferSize;
    float4 depthNormalTexture = SAMPLE_TEXTURE2D_LOD(_CameraDepthNormalTexture, sampler_point_clamp, screenUV, 0);
    //depth of the DepthNormal is not realiabe, maybe sample Depth Texture instead
    float4 depthTexture = SAMPLE_DEPTH_TEXTURE_LOD(_CameraDepthTexture, sampler_point_clamp, screenUV, 0);
    float3 bufferNormal = DecodeViewNormalStereo(depthNormalTexture);
    //float bufferDepth = DecodeFloatRG(depthNormalTexture.zw);
    float bufferDepth = depthTexture.r;
    //Linear01Depth get [0,1] depth, LinearEyeDepth get [near, far] depth
    float linearDepth = LinearEyeDepth(bufferDepth, _ZBufferParams);
    float linear01Depth = Linear01Depth(bufferDepth, _ZBufferParams);
    float3 p, pr, pl, pt, pb;
    //scale this tiny tiny delta
    float2 delta = invBufferSize * 1.2;
    p = UvToViewSpace(screenUV, linear01Depth);
    linear01Depth = Linear01Depth(SAMPLE_DEPTH_TEXTURE_LOD(_CameraDepthTexture, sampler_point_clamp, screenUV + float2(delta.x, 0), 0), _ZBufferParams);
    pr = UvToViewSpace(screenUV + float2(delta.x, 0), linear01Depth);
    linear01Depth = Linear01Depth(SAMPLE_DEPTH_TEXTURE_LOD(_CameraDepthTexture, sampler_point_clamp, screenUV + float2(-delta.x, 0), 0), _ZBufferParams);
    pl = UvToViewSpace(screenUV + float2(-delta.x, 0), linear01Depth);
    linear01Depth = Linear01Depth(SAMPLE_DEPTH_TEXTURE_LOD(_CameraDepthTexture, sampler_point_clamp, screenUV + float2(0, delta.y), 0), _ZBufferParams);
    pt = UvToViewSpace(screenUV + float2(0, delta.y), linear01Depth);
    linear01Depth = Linear01Depth(SAMPLE_DEPTH_TEXTURE_LOD(_CameraDepthTexture, sampler_point_clamp, screenUV + float2(0, -delta.y), 0), _ZBufferParams);
    pb = UvToViewSpace(screenUV + float2(0, -delta.y), linear01Depth);
    //get view space position delta of the point p in screen space
    float3 dpdu = MinDiff(p, pr, pl);
    float3 dpdv = MinDiff(p, pt, pb) * _CameraBufferSize.x * _CameraBufferSize.w;
    float3 geometricNormal = normalize(cross(dpdu, dpdv));
    float3 shadingNromal = normalize(bufferNormal);
    //from position to camera
    float3 viewVector = normalize(-p);
    float2 RAT = lerp(float2(aoRadius, thickness), fadeParams.zw, ComputeDistanceFade(abs(p.z)).xx);
    float radius = RAT.x;
    float thickness = RAT.y;
    //divide by Ns+1 so that the farthest samples are not fully attenuated
    float stepRadius = (max(min((radius * _HalfProjScale) / abs(p.z), 512), (float)numSlice)) / ((float)numSlice + 1);
    float noiseOffset = SpatialOffset(screenUV * bufferSize);
    float noiseDirection = SpatialDirection(screenUV * bufferSize);
    float ao = 0.0;
    float ro = 0.0;
    //slice rotation per angle
    float angle;
    float3 sliceDir;
    //plane is a half round slice of hemisphere, its normal can be calculated by cross product of vieDir and sliceDir cause all hemisphere is face to view
    float3 planeNormal, planeTangent;
    //cos weighting normal projection
    float3 normalSliceProj;
    float normalProjLength;
    //angle of view and projected normal
    float cos_gamma, gamma;
    //h1, h2 is positive and negative max occluded angle
    float3 h1, h2;
    float2 h, H, h1h2, h1h2Length, falloff;
    float2 uvOffset;
    float4 uvSlice;
    //bent normal use for reflection occlusion
    float bentAngle;
    float3 bentNormal;
    //hemisphere is along to view, and sample in several directions
    [loop]
    for (int i = 0; i < sampleCount; i++) {
        //rotate from 0 to PI makes a full circle
        angle = (i + noiseDirection) * (PI / (float)sampleCount);
        sliceDir = float3(float2(cos(angle), sin(angle)), 0);
        //NOTE : vecort order of the cross product MAKE SENSE(positive or negative dir)
        planeNormal = normalize(cross(sliceDir, viewVector));
        planeTangent = cross(viewVector, planeNormal);
        //geometric normal projected on the slice, by Gram－Schmidt methods
        normalSliceProj = geometricNormal - planeNormal * dot(geometricNormal, planeNormal);
        normalProjLength = length(normalSliceProj);
        //calculate a gamma angle to clamp h1&h2 later in the normal hemispehre(instead of view hemisphere)
        cos_gamma = clamp(dot(normalize(normalSliceProj), viewVector), -1, 1);
        //it is sigened
        gamma = -sign(dot(normalSliceProj, planeTangent)) * acos(cos_gamma);
        //init h params
        h = -1;
        [loop]
        for (int j = 0; j < numSlice; j++) {
            //sample slices by steps marching(imagine sample points is climbing on the sphere surfacce and projected in the radius)
            uvOffset = sliceDir.xy * invBufferSize * max(stepRadius * (j + noiseOffset), 1 + j);
            uvSlice = screenUV.xyxy + float4(uvOffset.xy, -uvOffset.xy);
            //calculate h1&h2 vectors
            float depth = SAMPLE_DEPTH_TEXTURE_LOD(_CameraDepthTexture, sampler_point_clamp, uvSlice.xy, 0);
            depth = Linear01Depth(depth, _ZBufferParams);
            h1 = UvToViewSpace(uvSlice.xy, depth) - p;
            depth = SAMPLE_DEPTH_TEXTURE_LOD(_CameraDepthTexture, sampler_point_clamp, uvSlice.zw, 0);
            depth = Linear01Depth(depth, _ZBufferParams);
            h2 = UvToViewSpace(uvSlice.zw, depth) - p;
            //calculate distance falloff
            h1h2 = float2(dot(h1, h1), dot(h2, h2));
            falloff = saturate(h1h2 * (2 / (radius * radius)));
            //count angle of h1&h1 in one params
            h1h2Length = rsqrt(h1h2);
            H = float2(dot(h1, viewVector), dot(h2, viewVector)) * h1h2Length;
            h = (H.xy > h.xy) ? lerp(H, h, falloff) : lerp(H, h, thickness);
        }
        h = acos(clamp(h, -1, 1));
        //clamp h with the normal hemisphere
        h.x = gamma + max(-h.x - gamma, -PI / 2);
        h.y = gamma + min(h.y - gamma, PI / 2);
        //construct bent normal
        bentAngle = (h.x + h.y) * 0.5;
        bentNormal += viewVector * cos(bentAngle) - planeTangent * sin(bentAngle);
        //integrate occlusion
        ao += normalProjLength * IntegrateArc_CosineWeighting(h, gamma);
    }
    ao = saturate(pow(ao / (float)sampleCount, gtaoStrength));
    bentNormal = normalize(normalize(bentNormal) - viewVector * 0.5);
    //calculate reflection occlusion
    float4 specular = SAMPLE_TEXTURE2D_LOD(_CameraSpecularTexture, sampler_linear_clamp, screenUV, 0);
    float roughness = specular.a;
    float3 reflectVector = reflect(viewVector, geometricNormal);
    ro = ReflectionOcclusion(bentNormal, reflectVector, roughness, 0.5);

    AmbientOcclusionRT[id.xy] = float3(ao, pow2(ro), linearDepth);
}

float3 GetSource(float2 screenUV) {
    return SAMPLE_TEXTURE2D_LOD(aoResult, sampler_linear_clamp, screenUV, 0).rgb;
}

float GetDepth(float2 screenUV) {
    return SAMPLE_DEPTH_TEXTURE_LOD(_CameraDepthTexture, sampler_point_clamp, screenUV, 0).r;
}

float3 GetNormal(float2 screenUV) {
    float4 depthNormal = SAMPLE_TEXTURE2D_LOD(_CameraDepthNormalTexture, sampler_point_clamp, screenUV, 0);
    float3 normal = DecodeViewNormalStereo(depthNormal);
    return normal;
}

float CompareNormal(float3 normal0, float3 normal1) {
    return smoothstep(filterFactor, 1.0, dot(normal0, normal1));
}

float2 GetValidUV(float2 uv, float2 delta) {
    float2 deltaUV = uv + delta;
    if (deltaUV.x > 1 || deltaUV.y > 1 || deltaUV.x < 0 || deltaUV.y < 0) {
        return uv;
    }
    else {
        return deltaUV;
    }
}

//blur with full resolution
[numthreads(8, 8, 1)]
void NormalBilateralFilter0(uint3 id : SV_DispatchThreadID) {
    float2 delta = filterRadius * _CameraBufferSize.xy;
    //[0,_CameraBufferSize-1] -> screen [0,1] uv
    float2 uv = id.xy / _CameraBufferSize.zw;
    float2 uv0a = GetValidUV(uv, - delta);
    float2 uv0b = GetValidUV(uv, + delta);
    float2 uv1a = GetValidUV(uv, - 2.0 * delta);
    float2 uv1b = GetValidUV(uv, + 2.0 * delta);
    float2 uv2a = GetValidUV(uv, - 3.0 * delta);
    float2 uv2b = GetValidUV(uv, + 3.0 * delta);
    //get normal
    float3 normal = GetNormal(uv);
    float3 normal0a = GetNormal(uv0a);
    float3 normal0b = GetNormal(uv0b);
    float3 normal1a = GetNormal(uv1a);
    float3 normal1b = GetNormal(uv1b);
    float3 normal2a = GetNormal(uv2a);
    float3 normal2b = GetNormal(uv2b);
    //get source
    float3 source = GetSource(uv);
    float3 source0a = GetSource(uv0a);
    float3 source0b = GetSource(uv0b);
    float3 source1a = GetSource(uv1a);
    float3 source1b = GetSource(uv1b);
    float3 source2a = GetSource(uv2a);
    float3 source2b = GetSource(uv2b);
    //calculate weight
    float w = 0.37004005286;
    float w0a = CompareNormal(normal, normal0a) * 0.31718061674;
    float w0b = CompareNormal(normal, normal0b) * 0.31718061674;
    float w1a = CompareNormal(normal, normal1a) * 0.19823788546;
    float w1b = CompareNormal(normal, normal1b) * 0.19823788546;
    float w2a = CompareNormal(normal, normal2a) * 0.11453744493;
    float w2b = CompareNormal(normal, normal2b) * 0.11453744493;
    float3 result = w * source;
    result += w0a * source0a;
    result += w0b * source0b;
    result += w1a * source1a;
    result += w1b * source1b;
    result += w2a * source2a;
    result += w2b * source2b;
    result = result / (w + w0a + w0b + w1a + w1b + w2a + w2b);
    Filtered0[id.xy] = result;
}

//blur with full resolution
[numthreads(8, 8, 1)]
void NormalBilateralFilter1(uint3 id : SV_DispatchThreadID) {
    float2 delta = filterRadius * _CameraBufferSize.xy;
    //[0,_CameraBufferSize-1] -> screen [0,1] uv
    float2 uv = id.xy / _CameraBufferSize.zw;
    float2 uv0a = uv - delta;
    float2 uv0b = uv + delta;
    float2 uv1a = uv - 2.0 * delta;
    float2 uv1b = uv + 2.0 * delta;
    float2 uv2a = uv - 3.0 * delta;
    float2 uv2b = uv + 3.0 * delta;
    //get normal
    float3 normal = GetNormal(uv);
    float3 normal0a = GetNormal(uv0a);
    float3 normal0b = GetNormal(uv0b);
    float3 normal1a = GetNormal(uv1a);
    float3 normal1b = GetNormal(uv1b);
    float3 normal2a = GetNormal(uv2a);
    float3 normal2b = GetNormal(uv2b);
    //get source
    float3 source = GetSource(uv);
    float3 source0a = GetSource(uv0a);
    float3 source0b = GetSource(uv0b);
    float3 source1a = GetSource(uv1a);
    float3 source1b = GetSource(uv1b);
    float3 source2a = GetSource(uv2a);
    float3 source2b = GetSource(uv2b);
    //calculate weight
    float w = 0.37004005286;
    float w0a = CompareNormal(normal, normal0a) * 0.31718061674;
    float w0b = CompareNormal(normal, normal0b) * 0.31718061674;
    float w1a = CompareNormal(normal, normal1a) * 0.19823788546;
    float w1b = CompareNormal(normal, normal1b) * 0.19823788546;
    float w2a = CompareNormal(normal, normal2a) * 0.11453744493;
    float w2b = CompareNormal(normal, normal2b) * 0.11453744493;
    float3 result = w * source;
    result += w0a * source0a;
    result += w0b * source0b;
    result += w1a * source1a;
    result += w1b * source1b;
    result += w2a * source2a;
    result += w2b * source2b;
    result = result / (w + w0a + w0b + w1a + w1b + w2a + w2b);
    Filtered1[id.xy] = result;
}

void GetAO_RO_Depth(float2 uv, inout float2 AO_RO, inout float AO_Depth) {
    float3 SSAOTexture = SAMPLE_TEXTURE2D_LOD(aoResult, sampler_linear_clamp, uv, 0);
    AO_RO = SSAOTexture.xy;
    AO_Depth = SSAOTexture.z;
}

float CrossBilateralWeight(float r, float d, float d0) {
    float blurSigma = (float)kernelSize * 0.5;
    float blurFalloff = 1 / (2 * blurSigma * blurSigma);
    float dz = (d0 - d) * _ProjectionParams.z * filterFactor;
    return exp2(-r * r * blurFalloff - dz * dz);
}

void ProcessSample(float3 AO_RO_Depth, float r, float d0, inout float2 totalAORO, inout float totalW) {
    float w = CrossBilateralWeight(r, d0, AO_RO_Depth.z);
    totalW += w;
    totalAORO += w * AO_RO_Depth.xy;
}

void ProcessRadius(float2 uv0, float2 deltaUV, float d0, inout float2 totalAORO, inout float totalW) {
    float r = 1.0;
    float z = 0.0;
    float2 uv = 0.0;
    float2 AO_RO = 0.0;
    for (; r <= kernelSize / 2; r += 1) {
        uv = uv0 + r * deltaUV;
        GetAO_RO_Depth(uv, AO_RO, z);
        ProcessSample(float3(AO_RO, z), r, d0, totalAORO, totalW);
    }
    for (; r <= kernelSize; r += 2) {
        uv = uv0 + (r + 0.5) * deltaUV;
        GetAO_RO_Depth(uv, AO_RO, z);
        ProcessSample(float3(AO_RO, z), r, d0, totalAORO, totalW);
    }
}

//adaption bilateral blur from battlefield3 of frostbite
[numthreads(8, 8, 1)]
void AdaptionBilateralUpSample(uint3 id : SV_DispatchThreadID) {
    float2 screenUV = id.xy / _CameraBufferSize.zw;
    float2 samplerSize = filterRadius * _CameraBufferSize.xy;
    float4 depthTexture = SAMPLE_DEPTH_TEXTURE_LOD(_CameraDepthTexture, sampler_point_clamp, screenUV, 0);
    float bufferDepth = depthTexture.r;
    //Linear01Depth get [0,1] depth, LinearEyeDepth get [near, far] depth
    float EyeDepth = LinearEyeDepth(bufferDepth, _ZBufferParams);
    float Eye01Depth = Linear01Depth(bufferDepth, _ZBufferParams);
    //up sample aoro
    float2 TopLeft_Color = GetSource(screenUV).rg;
    float2 TopRight_Color = GetSource(screenUV + (float2(0.0, 1.0) * samplerSize)).rg;
    float2 BottomLeft_Color = GetSource(screenUV + (float2(1.0, 0.0) * samplerSize)).rg;
    float2 BottomRight_Color = GetSource(screenUV + (float2(1.0, 1.0) * samplerSize)).rg;
    //up sample depth
    float TopLeft_Depth = LinearEyeDepth(GetDepth(screenUV), _ZBufferParams);
    float TopRight_Depth = LinearEyeDepth(GetDepth(screenUV + (float2(0.0, 1.0) * samplerSize)), _ZBufferParams);
    float BottomLeft_Depth = LinearEyeDepth(GetDepth(screenUV + (float2(1.0, 0.0) * samplerSize)), _ZBufferParams);
    float BottomRight_Depth = LinearEyeDepth(GetDepth(screenUV + (float2(1.0, 1.0) * samplerSize)), _ZBufferParams);

    float4 offsetDepths = float4(TopLeft_Depth, TopRight_Depth, BottomLeft_Depth, BottomRight_Depth);
    float4 weights = saturate(1.0 - abs(offsetDepths - EyeDepth));
    float2 fractCoord = frac(screenUV * samplerSize);
    float2 filteredX0 = lerp(TopLeft_Color * weights.x, TopRight_Color * weights.y, fractCoord.x);
    float2 filteredX1 = lerp(BottomRight_Color * weights.w, BottomLeft_Color * weights.z, fractCoord.x);
    float2 filtered = lerp(filteredX0, filteredX1, fractCoord.y);
    UpSampleRT[id.xy] = float3(filtered, EyeDepth);
}

//blur with full resolution
[numthreads(8, 8, 1)]
void AdaptionBilateralFilter0(uint3 id : SV_DispatchThreadID) {
    float2 uv = id.xy / _CameraBufferSize.zw;
    float2 deltaUV = filterRadius * _CameraBufferSize.xy;
    float depth;
    float2 totalAORO;
    GetAO_RO_Depth(uv, totalAORO, depth);
    float totalW = 1;
    ProcessRadius(uv, -deltaUV, depth, totalAORO, totalW);
    ProcessRadius(uv, deltaUV, depth, totalAORO, totalW);
    totalAORO /= totalW;
    Filtered0[id.xy] = float3(totalAORO, depth);
}

//blur with full resolution
[numthreads(8, 8, 1)]
void AdaptionBilateralFilter1(uint3 id : SV_DispatchThreadID) {
    float2 uv = id.xy / _CameraBufferSize.zw;
    float2 deltaUV = filterRadius * _CameraBufferSize.xy;
    float depth;
    float2 totalAORO;
    GetAO_RO_Depth(uv, totalAORO, depth);
    float totalW = 1;
    ProcessRadius(uv, -deltaUV, depth, totalAORO, totalW);
    ProcessRadius(uv, deltaUV, depth, totalAORO, totalW);
    totalAORO /= totalW;
    Filtered1[id.xy] = float3(totalAORO, depth);
}

[numthreads(8, 8, 1)]
void DebugAO(uint3 id : SV_DispatchThreadID) {
    float2 uv = id.xy / _CameraBufferSize.zw;
    float3 ao = Filtered1[id.xy].xxx;
    if (multipleBounce == 1) {
        float3 albedo = SAMPLE_TEXTURE2D_LOD(_CameraDiffuseTexture, sampler_linear_clamp, uv, 0);
        ao = MultiBounce(ao.r, albedo);
        debugResult[id.xy] = ao;
    }
    debugResult[id.xy] = ao;
}

[numthreads(8, 8, 1)]
void DebugRO(uint3 id : SV_DispatchThreadID) {
    float2 uv = id.xy / _CameraBufferSize.zw;
    float3 ro = Filtered1[id.xy].yyy;
    debugResult[id.xy] = ro;
}